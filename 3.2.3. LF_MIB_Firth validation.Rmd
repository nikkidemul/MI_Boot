---
title: "3.2.3. LF_MIB_Firth validation"
author: "N. de Mul"
date: "2023-11-10"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Firth Validation

For the bootstrap we will: 
- Take n bootstrap samples for each m imputation set
- Develop the logistf model on all 500 bootstrap datasets, for each imputation set 
- Test performance of the models on the bootstrap sample they were developed in and on the imputation set the bootstrap samples were derived from. 
- Difference in model performance = optimism 
- Pool performance measures 
- Save output in order to create the Heinzetables and stability plots later on. 



Perform bootstrap: 
```{r}
set.seed(21212)

#function for bootstrap: 
bootstrap <- function(dataimp, bootstrapn){
  
  #store output
  bootstraplist <- list()
  
  #set n for number of bootstrap samples 
  n <- bootstrapn
  
  #create loop
  for(i in 1:n){
    
    #make sure you sample with replacement 
    bootstrap <- sample_n(dataimp, nrow(dataimp), replace=TRUE)
    
    #store results in a list
    bootstraplist[[i]] <- bootstrap
  }
  return(bootstraplist)
}


#Then create the bootstrap samples for each imputed dataset. We will use lapply to loop over the list of imputed datasets: 

set.seed(21212)
bootstrapsimpF <- lapply(Final_impF, bootstrap, bootstrapn=500)

#save results: 
saveRDS(bootstrapsimpF, file=paste0("bootstrapsimpF", ".rds"))
   
```

Then develop the models: 

```{r}
modelfirth <- function(bootstrapsets, formula){
  
  #store output
  modelsbsF <- list()
  
  #create loop over imputation sets 
  for(i in 1:length(bootstrapsets)){
    
    #first extract list with 500 bootstraps from list with imputation sets 
    impcategories <- bootstrapsets[[i]]
    
    #store output of following loop
    models <- list()
    
    #then create loop within that imputation set number, over the 500 bootstrap samples it contains 
    for(j in 1:length(impcategories)){
      setbs <- impcategories[[j]] #<<- is used because logistf cannot look into function environment so we have to save it in the general enviroment. 
      
    #create models using logistf
    fullV <- logistf(formula, data=setbs)
    bwmodelV <- backward(fullV, slstay=0.157, printwork=F, trace=F) #slsstay is your p value for keeping a variable in the model, we took the AIC criterion. Printwork = T will show you all steps, but slows the process. 
    bwmodelFV <- flic(bwmodelV)
    modelsF[[j]] <- bwmodelFV
    }
    modelsbsF[[i]] <- modelsF
  }
  return(modelsbsF)
}


#Develop the bootstrap models and save them into modelsbsimp 

###ORIGINAL function but provided an error that it could not locate data. 
# set.seed(21212)
# modelsbsimpF <- modelfirth(bootstrapsimpF, formula=outcome~age+
#                          open+
#                          T34+
#                          dummy_Neotx1+
#                          transhiatal+
#                          fev1_compl+
#                          dummy_Hblow+
#                          comorb_dm+
#                          gender+
#                          eGFR+
#                          dummy_ASA34+
#                          dummy_Hbhigh+
#                          comorb_cardiovasc+
#                          comorb_hypertension+
#                          smoking+
#                          dummy_Neotx3+
#                          bmi+
#                          tiff_compl)
# 
# #save results for later use 
# saveRDS(modelsbsimpF,file=paste0("modelsbsimpF",".rds"))



###Therefore now used the function used on the imputation sets, but applied to the list within that list. 
backwardimp <- function(impsets, formula){
  
  #store output
  models <- list()
  
  #loop over imputed datasets
  for (i in 1:length(impsets)){
    impset <<- impsets[[i]] #<<- is used because it seems that logistf cannot look into function environment so we had to save it in the general enviroment. NB: if you reuse this script, make sure you don't use impset in another function, because it is invisibly stored in general environment and R will look into that first! 
    
    #create models using logistf
    full <- logistf(formula, data=impset)
    bwmodel <- backward(full, slstay=0.157, trace=F, printwork=F, data=impset) #specify AIC criterion, trace T will print everything, which can be usefull but it is faster to set to F. 
    bwmodelF <- flic(bwmodel)
    models[[i]] <- bwmodelF
  }
  return(models)
}

modelsbsimpF <- lapply(bootstrapsimpF, backwardimp, formula=outcome~age+
                         open+
                         T34+
                         dummy_Neotx1+
                         transhiatal+
                         fev1_compl+
                         dummy_Hblow+
                         comorb_dm+
                         gender+
                         eGFR+
                         dummy_ASA34+
                         dummy_Hbhigh+
                         comorb_cardiovasc+
                         comorb_hypertension+
                         smoking+
                         dummy_Neotx3+
                         bmi+
                         tiff_compl)
```

Then calculate corrected performance in steps. 

First, we develop the nullmodels in each individual bootstrap sample. We need these to later calculate the Index of Prediction Accuracy, conform we did for apparent model performance. 

```{r}
nullmodels <- function(bootstraps){
  
  #store output
  nullmodels <- list()
  
  #loop over imputed datasets
  for (i in 1:length(bootstraps)){
    set <- bootstraps[[i]]
    #create models
    null <- logistf(outcome~1, data=set)
    nullmodels[[i]] <- null
  }
  return(nullmodels)
}

#develop the nullmodels. I have not names bootstrapimp to Firth, because these are the same for GLM. 
nullmodelsF <- lapply(bootstrapsimp, nullmodels)


#logit function in order to obtain a logit AUC that will make it possible to later pool AUCs. 

logit <- function(x){log(x / (1-x))}

#estimate log c-statistic in elke imp set zodat we kunnen poolen en terug kunnen transformeren. 
compute_c_stat_log_imp <- function(obs_outcome, pred_outcome, method="delong", direction="<"){
  
  #estimate c_stat
  c_stat <- pROC::auc( response = obs_outcome,
                       predictor = pred_outcome,
                       direction = direction,
                       method = method)
  
  #estimate confidence intervals appropriately
  variance <- pROC::var(c_stat)
  logit_se <- sqrt(variance)/(c_stat*(1-c_stat))
  return(c (auc=c_stat, logitauc = logit(as.numeric(c_stat)),
            logit_se=logit_se))
}


#Then the performance measures script that is the same as for apparent performance, but containing an extra loop (we now have a list of 10 - corresponding to the 10 imputation sets, and these contain lists with 500 bootstrap samples and models. We also still have the list with 10 original imputation sets). We now want to calculate bootstrapmodel performance in the bootstrap sample the model was developed in, and in the original imputation set. 

impPerfBSF <- function(imputatiesets, bootstrapsets, bsmodels, bsnullmodels){
  
  #store output as two seperate lists, one for performance in the bootstrap samples and one for performance in the original imputation sets. 
  performancetablesbs <- list()
  performancetablesimp <- list()
  
  
  #first create a loop length of imputation sets, creating the imputation sets, the list with their respective bootstrap samples (individual), list of models for each set and list of null models for each set.  
  for(i in 1:length(imputatiesets)){
  
    imputatieset <- imputatiesets[[i]]
    bootstraplist <- bootstrapsets[[i]]
    modelbslist <- bsmodels[[i]]
    nullmodelbslist <- bsnullmodels[[i]]
    
    #then loop within the bootstrap samples lists, using the models and the nullmodels. 
    for(j in 1:length(bootstraplist)){
      bootstrapdf <- bootstraplist[[j]]
      modelbs <- modelbslist[[j]]
    nullmodelbs <- nullmodelbslist[[j]]
    bootstrapdf$pred <- predict(modelbs, type="response", newdata=bootstrapdf)
    
    ### Bootstrap on bootstrap performance
    #auc bootstrapmodel in bootstrapsample
    bootstrap <- compute_c_stat_log_imp(obs_outcome=bootstrapdf$outcome, pred_outcome=bootstrapdf$pred)
    c_statlogdf <- as.data.frame(bootstrap)
    c_statlogdf <- c_statlogdf %>% rownames_to_column("performanceM")
    
    #use null models for calculating IPA (bootstrapmodel in bootstrap sample)
    bootstrapdf$outcome <- as.numeric(bootstrapdf$outcome)-1
    bootstrapdf$prednull <- predict(nullmodelbs, type="response", newdata=bootstrapdf)
    null <- val.prob(bootstrapdf$prednull, as.numeric(bootstrapdf$outcome), pl=FALSE)
    performancenulldf <- as.data.frame(null)
    performancenulldf <- performancenulldf %>% rownames_to_column("performanceM")
    performancenulldf <- performancenulldf %>% filter(!is.na(performanceM))
    
    #andere performance measures bootstrapmodels in bootstrapsample 
    # first using val.prob
    bootstrap <- val.prob(bootstrapdf$pred, as.numeric(bootstrapdf$outcome), pl=FALSE)
    bootstrapdf2 <- data.frame(bootstrap)
    bootstrapdf2 <- bootstrapdf2 %>% rownames_to_column("performanceM")
    
    #then calculating IPA and O:E ratio. Storing all these measures as performanceM as well in order to later merge all the datasets. 
    add.data.IPAbs <- data.frame(performanceM = "IPA" , bootstrap = (1-((bootstrapdf2$bootstrap[bootstrapdf2$performanceM=="Brier"])/(performancenulldf$null[performancenulldf$performanceM=="Brier"])))) 
    add.data.IPAbslogit <- data.frame(performanceM="logitIPA", bootstrap = logit(as.numeric(add.data.IPAbs$bootstrap)))
      add.dataOE <- data.frame(performanceM = "OE", bootstrap = as.numeric((mean(as.numeric(bootstrapdf$outcome))))/(mean(bootstrapdf$pred)))
    bootstrapdf3 <- bind_rows(bootstrapdf2, c_statlogdf, add.data.IPAbs, add.data.IPAbslogit, add.dataOE) 
  
    #merge everything together 
    bootstrapdf4 <- bootstrapdf3 %>% filter(performanceM=="auc"|performanceM=="Brier" | performanceM=="logitauc"|performanceM=="logit_se"|performanceM=="IPA"|performanceM=="logitIPA"|performanceM=="Slope" | performanceM=="Intercept" | performanceM=="OE") 
    
    ### IMPUTATION set
    #repeat this process for testing performance on the original imputation set
    imputatieset$pred <- predict(modelbs, type="response", newdata=imputatieset)
    
    #auc
    imp <- compute_c_stat_log_imp(obs_outcome=imputatieset$outcome, pred_outcome=imputatieset$pred)
    c_statlogdfimp <- as.data.frame(imp)
    c_statlogdfimp <- c_statlogdfimp %>% rownames_to_column("performanceM")
    
    #null model predicted values to calculate IPA later
    imputatieset$prednull <- predict(nullmodelbs, type="response", newdata=imputatieset)
    null <- val.prob(imputatieset$prednull, as.numeric(imputatieset$outcome), pl=FALSE)
    impnulldf <- as.data.frame(null)
    impnulldf <- impnulldf %>% rownames_to_column("performanceM")
    impnulldf <- impnulldf %>% filter(!is.na(performanceM))
    
    #performance measures using val.prob
    imp <- val.prob(imputatieset$pred, as.numeric(imputatieset$outcome), pl=FALSE)
    impdf <- data.frame(imp)
    impdf <- impdf %>% rownames_to_column("performanceM")
    
    #performance measures calculated by hand (IPA and O:E)
    add.data.IPAimp <- data.frame(performanceM = "IPA" , imp=(1-((impdf$imp[impdf$performanceM=="Brier"])/(impnulldf$null[impnulldf$performanceM=="Brier"])))) 
    add.data.IPAimplogit <- data.frame(performanceM="logitIPA", imp = logit(as.numeric(add.data.IPAimp$imp)))
     add.dataOEimp <- data.frame(performanceM = "OE", imp = as.numeric(mean(as.numeric(imputatieset$outcome)))/(mean(imputatieset$pred)))
    impdf2 <- bind_rows(impdf, c_statlogdfimp, add.data.IPAimp, add.data.IPAimplogit, add.dataOEimp) 
     
    #merge everything together              
    impdf3 <- impdf2 %>% filter(performanceM=="auc"|performanceM=="logitauc"|performanceM=="logit_se"| performanceM=="Brier" | performanceM=="IPA"|performanceM=="logitIPA"|performanceM=="Slope" | performanceM=="Intercept" | performanceM== "OE")
    
    performance <- merge(bootstrapdf3, impdf3, by="performanceM")
    performance$optimism <- performance$bootstrap - performance$imp
    
    #store the performance measures for individual bootstrap models
    performancetablesbs[[j]] <- performance
    
    }
    
    #store the bootstrap lists in the imputation list
    performancetablesimp[[i]] <- performancetablesbs
    
  }
  return(performancetablesimp)
}


#calculate performance
testperformanceF <- impPerfBSF(imputatiesets=Final_impF, bootstrapsets=bootstrapsimp, bsmodels=modelsbsimpF, bsnullmodels=nullmodelsF)

#save results
saveRDS(testperformanceF, file=paste0("listperformanceF",".rds"))


#unlist to stacked dataframe, which we will need to calculate 95% confidence intervals.  
performance_rowbindtotalF <- bind_rows(testperformanceF) 

#save results
saveRDS(performance_rowbindtotalF, file=paste0("performance_rowbindtotalF",".rds"))

```

NB: because we had to convert to numeric for Firth's logistic regression, R converts outcome to 1 and 2, therefore the O:E ratio is getting really weird (>2). We substract one in order to prevent that. You can also change that before using the function. 


Then we pool the performance measures: 

```{r}
mean_performanceF <- bind_rows(performance_rowbindtotalF)

#take the mean of the performance measures 
mean_performanceF <- mean_performanceF %>% group_by(performanceM) %>% summarise(across(where(is.numeric), mean, na.rm=TRUE), .groups="drop")

#then convert back to plogis for AUC
mean_performanceF$bootstrap[mean_performanceF$performanceM=="logitauc"] <-   plogis(mean_performanceF$bootstrap[mean_performanceF$performanceM=="logitauc"])
mean_performanceF$imp[mean_performanceF$performanceM=="logitauc"] <- plogis(mean_performanceF$imp[mean_performanceF$performanceM=="logitauc"]) 

saveRDS(mean_performanceF,file=paste0("meanperformanceF20240223",".rds"))
```

Calculate optimism correct performance measures (subtract optimism from apparent performance): 

Calibration Intercept: 
```{r}
finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Intercept"] - mean_performanceF$optimism[mean_performanceF$performanceM=="Intercept"]
```
Calibration slope: 
```{r}
finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Slope"] - mean_performanceF$optimism[mean_performanceF$performanceM=="Slope"]
```

Brier: 
```{r}
finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Brier"] - mean_performanceF$optimism[mean_performanceF$performanceM=="Brier"]
```


Now use the stacked performance set (performance_rowbindtotalF) to calculate confidence intervals of calibration intercept and slope. 

Concerning the 95% CI's: 
* Calibration Intercept and Slope are not given in apparent performance. We will calculate the percentiles in the bootstrap sets. 
* Concerning point estimate AUC: use estimated variance calculated using Rubin's Rules. 
* Concerning IPA and O:E ratio: calculate the average estimate over the 10 datasets, per impset over the 500 bootstraps (bs model tested on bs set) and get the percentiles. We divide the difference in percentiles by 3.92 (2x1.96) which yields the SE per impset. We then pool these results using Rubin's Rules and use these to calculate the 95% CI of the apparent point estimates. We then shift the interval towards the point estimates of the corrected performance using the same optimism correction as for the point estimates. 


First the calibration intercept and slope percentiles: 

```{r}
# calibrationF <- performance_rowbindtotalF %>% filter(performanceM=="Intercept" | performanceM=="Slope")
# calibrationbsF <- calibrationF %>% select(performanceM, bootstrap)
# calibrationimpF <- calibrationF %>% select(performanceM, imp) #ik denk dat ik deze moet hebben toch? 
# calibrationimpintF <- calibrationimpF %>% filter(performanceM=="Intercept")
# calibrationimpSF <- calibrationimpF %>% filter(performanceM=="Slope")
# 
# calibrationbsintF <- calibrationbsF %>% filter(performanceM == "Intercept")
# calibrationbsSF <- calibrationbsF %>% filter(performanceM == "Slope")
# var(calibrationimpintF$imp) #variance is 0.006551138 
```

For calibration measures we decided to use percentiles surrounding the performance measures. 

```{r}
# #Intercept
# quantile( calibrationbsintF$bootstrap, probs = 0.975) - -0.090764414 #0.2268
# quantile( calibrationbsintF$bootstrap, probs = 0.025) - -0.090764414#-0.1056
# 
# #Slope
# quantile(calibrationbsSF$bootstrap, probs=0.975) - 0.233037387 
# quantile(calibrationbsSF$bootstrap, probs=0.025) -0.233037387

```

Die 95% confidence interval van calibratie slope klopt niet - nu alleen imp mee -  ook bootstrap? --> Intercept en slope 95%CI nu berekend zoals hieronder. 

Then the 95% confidence intervals of IPA en O:E ratio. The procedure is the same. IPA will we scripted first. 
* We need the rubins_rules script for pooling standard errors again. 
* We use the performancelongF dataframe from 3.1.3. That contains all performance measures in a stacked dataframe. We need that for pooling the SE's of the point estimates. 
* We calculate the 95% confidence intervals using the bootstrap models on the bootstrap samples. 
* We then take the percentiles again, upper and lower (0.025, 0.975), take the difference and divide by 3.92 --> SE per imputationset. 
* We then save these SE's and save them as output and use these to pool them using Rubins'Rules. 

```{r}
#compute variance over imputations using Rubins rules

rubins_rules_var <- function(estimates, ses, n_imputed_sets){
  
  #within study variance
  within_var <- mean(ses^2)
  
  #between study variance
  theta_bar <- mean(estimates)
  
  between_var <- sum( (estimates - theta_bar)^2)/n_imputed_sets
  
  #total variance
  total_var <- within_var + between_var + between_var/n_imputed_sets 
  
  return(total_var)
}

# First IPA: 

performanceIPA <- performancelongF %>% filter(performanceM=="IPA") #these are the 10 point estimates we need for the SE calculation. 

# The SE's have to be calculated per imputationset, and then later pooled. 


getsesIPA <- function(performancelistbs){
  
  #store output
  selist <- list() 
  
  #loop over the 10 imputed sets with bootstraps 
  for(i in 1:length(performancelistbs)){
    
    #take the list per imputation set and bind rows to create a stacked dataframe 
    performancelist <- performancelistbs[[i]]
    performancebinded <- bind_rows(performancelist) 
    
    #chose the right performance measure 
    performancemeasure <- performancebinded %>% filter(performanceM=="IPA") 
    
    #filter on bootstrap values only (we use bootstrap model on bootstrap sample performance)
    performancemeasure2 <- performancemeasure %>% select(performanceM, bootstrap)
    
    #calculate percentiles and subsequent the standard errors 
    upper <- quantile(performancemeasure2$bootstrap, probs=0.975)
    lower <- quantile(performancemeasure2$bootstrap, probs=0.025)
    se <- (upper-lower)/3.92
    selist[[i]] <- se
  }
  return(selist)
}

#then use this function to get the standard errors, and bind the standard errors of the 10 imputation sets together. 
IPAse <- getsesIPA(performancelistbs=testperformanceF)
IPAse2 <- bind_rows(IPAse)
colnames(IPAse2)[1] <- "se"

#Then pool these ses using rubins rules

IPApunt <- performancelongF %>% filter(performanceM=="IPA")
SEIPA <- rubins_rules_var(estimates=IPApunt$performanceimp, ses=IPAse2$se, n_imputed_sets=10)
SEIPA  

#then use this value to calculate the confidence intervals 

IPAupperF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="IPA"] + 1.96*sqrt(SEIPA)
IPAlowerF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="IPA"] - 1.96*sqrt(SEIPA)


#optimism corrected is original value IPA - corrected value IPA, using the same formula to calculate the confidence intervals. 
correctedIPA <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="IPA"] - mean_performanceF$optimism[mean_performanceF$performanceM=="IPA"]

IPAupperFC <- correctedIPA + 1.96*sqrt(SEIPA)
IPAlowerFC <- correctedIPA - 1.96*sqrt(SEIPA)

```


Then we repeat this process for the observed-expected ratio: 

```{r}
performanceOE <- performancelongF %>% filter(performanceM=="OE") #get the 10 point estimates 

#Then calculate the SE's per imputation set 

getsesOE <- function(performancelistbs){
  
  #store output
  selist <- list() 
  
  #create loop
  for(i in 1:length(performancelistbs)){
    performancelist <- performancelistbs[[i]]
    performancebinded <- bind_rows(performancelist) 
    
    #chose performance measure
    performancemeasure <- performancebinded %>% filter(performanceM=="OE") 
    
    #filter on the bootstrap values only 
    performancemeasure2 <- performancemeasure %>% select(performanceM, bootstrap)
    
    #calculate percentiles and subsequent the 95% confidence intervals 
    upper <- quantile(performancemeasure2$bootstrap, probs=0.975)
    lower <- quantile(performancemeasure2$bootstrap, probs=0.025)
    se <- (upper-lower)/3.92
    selist[[i]] <- se
  }
  return(selist)
}

#get the standard errors and bind them together to subsequently pool them 
OEse <- getsesOE(performancelistbs=testperformanceF)
OEse2 <- bind_rows(OEse)
colnames(OEse2)[1] <- "se"

#pool ses using rubins rules
OEpunt <- performancelongF %>% filter(performanceM=="OE")
SEOE <- rubins_rules_var(estimates=OEpunt$performanceimp, ses=OEse2$se, n_imputed_sets=10)
SEOE 

#calculate confidence intervals: 
OEupperF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="OE"] + 1.96*sqrt(SEOE)
OElowerF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="OE"] - 1.96*sqrt(SEOE)

#corrected for optimism (same method as described for IPA): 
correctedOE <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="OE"] - mean_performanceF$optimism[mean_performanceF$performanceM=="OE"]

OEupperFC <- correctedOE + 1.96*sqrt(SEOE)
OElowerFC <- correctedOE - 1.96*sqrt(SEOE)
```

Repeated this process for the Brier score

```{r}
# Brier

performanceBrier <- performancelongF %>% filter(performanceM=="Brier") 

getsesBrier <- function(performancelistbs){
  selist <- list() 
  
  for(i in 1:length(performancelistbs)){
    performancelist <- performancelistbs[[i]]
    performancebinded <- bind_rows(performancelist) 
    
    performancemeasure <- performancebinded %>% filter(performanceM=="Brier") 
   
    performancemeasure2 <- performancemeasure %>% select(performanceM, bootstrap)
   
    upper <- quantile(performancemeasure2$bootstrap, probs=0.975)
    lower <- quantile(performancemeasure2$bootstrap, probs=0.025)
    se <- (upper-lower)/3.92
    selist[[i]] <- se
  }
  return(selist)
}

Brierse <- getsesBrier(performancelistbs=testperformanceF)
Brierse2 <- bind_rows(Brierse)
colnames(Brierse2)[1] <- "se"

Brierpunt <- performancelongF %>% filter(performanceM=="Brier")
SEBrier <- rubins_rules_var(estimates=Brierpunt$performanceimp, ses=Brierse2$se, n_imputed_sets=10)
SEBrier 

BrierupperF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Brier"] + 1.96*sqrt(SEBrier)
BrierlowerF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Brier"] - 1.96*sqrt(SEBrier)

#corrected: 
correctedBrier <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Brier"] - mean_performanceF$optimism[mean_performanceF$performanceM=="Brier"]


BrierupperFC <- correctedBrier + 1.96*sqrt(SEBrier)
BrierlowerFC <- correctedBrier - 1.96*sqrt(SEBrier)

```


```{r}
# Calibration Intercept

performanceI <- performancelongF %>% filter(performanceM=="Intercept") 

getsesI <- function(performancelistbs){
  selist <- list() 
  
  for(i in 1:length(performancelistbs)){
    performancelist <- performancelistbs[[i]]
    performancebinded <- bind_rows(performancelist) 
    
    performancemeasure <- performancebinded %>% filter(performanceM=="Intercept") 
   
    performancemeasure2 <- performancemeasure %>% select(performanceM, bootstrap)
   
    upper <- quantile(performancemeasure2$bootstrap, probs=0.975)
    lower <- quantile(performancemeasure2$bootstrap, probs=0.025)
    se <- (upper-lower)/3.92
    selist[[i]] <- se
  }
  return(selist)
}

Ise <- getsesI(performancelistbs=testperformanceF)
Ise2 <- bind_rows(Ise)
colnames(Ise2)[1] <- "se"

Ipoint <- performancelongF %>% filter(performanceM=="Intercept")
SEI <- rubins_rules_var(estimates=Ipoint$performanceimp, ses=Ise2$se, n_imputed_sets=10)
SEI # 5.338393e-05

IupperF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Intercept"] + 1.96*sqrt(SEI)
IlowerF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Intercept"] - 1.96*sqrt(SEI)

#corrected: 
correctedI <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Intercept"] - mean_performanceF$optimism[mean_performanceF$performanceM=="Intercept"]

IupperFC <- correctedI + 1.96*sqrt(SEI)
IlowerFC <- correctedI - 1.96*sqrt(SEI)
```


```{r}
# Calibration Slope

performanceSlope <- performancelongF %>% filter(performanceM=="Slope") 

getsesSlope <- function(performancelistbs){
  selist <- list() 
  
  for(i in 1:length(performancelistbs)){
    performancelist <- performancelistbs[[i]]
    performancebinded <- bind_rows(performancelist) 
    
    performancemeasure <- performancebinded %>% filter(performanceM=="Slope") 
   
    performancemeasure2 <- performancemeasure %>% select(performanceM, bootstrap)
   
    upper <- quantile(performancemeasure2$bootstrap, probs=0.975)
    lower <- quantile(performancemeasure2$bootstrap, probs=0.025)
    se <- (upper-lower)/3.92
    selist[[i]] <- se
  }
  return(selist)
}

Slopese <- getsesSlope(performancelistbs=testperformanceF)
Slopese2 <- bind_rows(Slopese)
colnames(Slopese2)[1] <- "se"

Slopepoint <- performancelongF %>% filter(performanceM=="Slope")
SESlope <- rubins_rules_var(estimates=Slopepoint$performanceimp, ses=Slopese2$se, n_imputed_sets=10)
SESlope # 5.338393e-05

SlopeupperF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Slope"] + 1.96*sqrt(SESlope)
SlopelowerF <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Slope"] - 1.96*sqrt(SESlope)

#corrected: 
correctedSlope <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Slope"] - mean_performanceF$optimism[mean_performanceF$performanceM=="Slope"]

SlopeupperFC <- correctedSlope + 1.96*sqrt(SESlope)
SlopelowerFC <- correctedSlope - 1.96*sqrt(SESlope)

```

AUC: 
```{r}
#corrected
mean_performanceF$performanceM <- ifelse(mean_performanceF$performanceM=="auc", "AUC", mean_performanceF$performanceM)

correctedAUC <- finalperformanceF$Meanperformance[finalperformanceF$performanceM=="AUC"] - (mean_performanceF$optimism[mean_performanceF$performanceM=="AUC"])


AUClowerF <- plogis(logit(finalperformanceF$Meanperformance[finalperformanceF$performanceM=="AUC"])-1.96*sqrt(total_var_performance_impF))
AUCupperF <- plogis(logit(finalperformanceF$Meanperformance[finalperformanceF$performanceM=="AUC"])+1.96*sqrt(total_var_performance_impF))

AUClowerFC <- AUClowerF - mean_performanceF$optimism[mean_performanceF$performanceM=="AUC"]
AUCupperFC <- AUCupperF - mean_performanceF$optimism[mean_performanceF$performanceM=="AUC"]
```


Create a table with original performance with upper en lower CI, apparent bootstrap, imp, optimism and corrected. 

```{r}
performanceM <- c("AUC", "OE", "Intercept", "Slope", "Brier", "IPA")
original <- c(finalperformanceF$Meanperformance[finalperformanceF$performanceM=="AUC"], finalperformanceF$Meanperformance[finalperformanceF$performanceM=="OE"], finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Intercept"], finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Slope"], finalperformanceF$Meanperformance[finalperformanceF$performanceM=="Brier"], finalperformanceF$Meanperformance[finalperformanceF$performanceM=="IPA"])
lowerO <- c(AUClowerF, OElowerF, IlowerF, SlopelowerF, BrierlowerF, IPAlowerF)
upperO <- c(AUCupperF, OEupperF, IupperF, SlopeupperF, BrierupperF, IPAupperF)
bootstrap <- c(mean_performanceF$bootstrap[mean_performanceF$performanceM=="logitauc"], mean_performanceF$bootstrap[mean_performanceF$performanceM=="OE"], mean_performanceF$bootstrap[mean_performanceF$performanceM=="Intercept"], mean_performanceF$bootstrap[mean_performanceF$performanceM=="Slope"], mean_performanceF$bootstrap[mean_performanceF$performanceM=="Brier"], mean_performanceF$bootstrap[mean_performanceF$performanceM=="IPA"])
imp <- c(mean_performanceF$imp[mean_performanceF=="logitauc"], mean_performanceF$imp[mean_performanceF=="OE"], mean_performanceF$imp[mean_performanceF=="Intercept"], mean_performanceF$imp[mean_performanceF=="Slope"], mean_performanceF$imp[mean_performanceF=="Brier"], mean_performanceF$imp[mean_performanceF=="IPA"])
optimism <- c(mean_performanceF$optimism[mean_performanceF$performanceM=="AUC"], mean_performanceF$optimism[mean_performanceF$performanceM=="OE"], mean_performanceF$optimism[mean_performanceF$performanceM=="Intercept"], mean_performanceF$optimism[mean_performanceF$performanceM=="Slope"], mean_performanceF$optimism[mean_performanceF$performanceM=="Brier"], mean_performanceF$optimism[mean_performanceF$performanceM=="IPA"])
corrected <- c(correctedAUC, correctedOE, correctedI, correctedSlope, correctedBrier, correctedIPA)
lowerC <- c(AUClowerFC, OElowerFC, IlowerFC, SlopelowerFC, BrierlowerFC, IPAlowerFC)
upperC <- c(AUCupperFC, OEupperFC, IupperFC, SlopeupperFC, BrierupperFC, IPAupperFC)


Fperformancetable <- as.data.frame(cbind(performanceM, original, lowerO, upperO, bootstrap, imp, optimism, corrected, lowerC, upperC))
Fperformancetable <- Fperformancetable %>% mutate(original = as.numeric(original)) %>% mutate(lowerO = as.numeric(lowerO)) %>% mutate(upperO = as.numeric(upperO)) %>% mutate(bootstrap = as.numeric(bootstrap)) %>% mutate(imp = as.numeric(imp)) %>% mutate(optimism = as.numeric(optimism)) %>% mutate(corrected=as.numeric(corrected)) %>% mutate(lowerC = as.numeric(lowerC)) %>% mutate(upperC = as.numeric(upperC))

saveRDS(Fperformancetable, paste0("Firthperformancetotal", ".rds"))
#nog even afronden op 2 decimalen
Fperformancetable2 <- Fperformancetable %>%mutate(across(is.numeric, round, digits=2))

saveRDS(Fperformancetable2, paste0("finalFirthperformance",".rds"))
```


```{r}
Fperformancetable2
```
























